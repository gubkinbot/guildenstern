{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6cb246c4984f9389c01a351cd562cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/175 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fde23cbb3f24f558b30063924dec8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/distilrubert-tiny-cased-conversational-v1 were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"DeepPavlov/distilrubert-tiny-cased-conversational-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9897cce71a6f43e7820163d47d390503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3df72ae90564b6a87704a47290d13e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50264, 768)\n",
       "  (wpe): Embedding(2048, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModel.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf9e6a4fb5a4e62bf304f8733d6474e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c7660eedbc482c9fe7bbfc90a13ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d83b2824c540eaa27e530ffad68a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/980k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"sberbank-ai/ruT5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"sberbank-ai/ruT5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пользователь 1: Привет, как дела?\n",
      "Пользователь 2: Привет, всё хорошо.\n",
      "Пользователь 1: Чем занимаешься?\n",
      "Пользователь 2: Спойлер нажми!\n",
      "Пользователь 3: На что ты учишь язык?\n",
      "Пользователь 4: Слушай\n",
      "========================================\n",
      "Пользователь 1: Привет, как дела?\n",
      "Пользователь 2: Привет, всё хорошо.\n",
      "Пользователь 1: Чем занимаешься?\n",
      "Пользователь 2: Вот я и вернулся!\n",
      "Пользователь 2: Здравствуйте, а ЧЕМУ ОТКРЫТ АКБ?\n",
      "Польз\n",
      "========================================\n",
      "Пользователь 1: Привет, как дела?\n",
      "Пользователь 2: Привет, всё хорошо.\n",
      "Пользователь 1: Чем занимаешься?\n",
      "Пользователь 2: Устроился в свою контору (помогите с поиском вакансий по запросу \"киндер\") В общей сложности на данный момент более\n",
      "========================================\n",
      "tensor([-0.0413, -0.0077, -0.0575])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Пользователь 1: Привет, как дела?\n",
    "Пользователь 2: Привет, всё хорошо.\n",
    "Пользователь 1: Чем занимаешься?\n",
    "\"\"\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    hypotheses = model.generate(\n",
    "        **inputs, \n",
    "        do_sample=True, num_return_sequences=3, num_beams=15,\n",
    "        repetition_penalty=2.5, temperature=1.9, top_k=500,\n",
    "        max_new_tokens=32, return_dict_in_generate=True, output_scores=True, early_stopping=True\n",
    "    )\n",
    "\n",
    "for t in hypotheses.sequences:\n",
    "    print(tokenizer.decode(t, skip_special_tokens=True))\n",
    "    print(\"========================================\")\n",
    "print(hypotheses.sequences_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Хорошо.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"\"\"Us1: Привет, как дела?\\n\\n\n",
    "Us2: Привет, всё хорошо.\\n\\n\n",
    "Us1: Чем занимаешься?\\n\\n\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(\"kjafsklja\", Iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"cointegrated/rut5-small-chitchat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Привет, ты как? Здорово. Так себе. А что случилось?</s>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"\"\"Привет, ты как?\\n\\n\n",
    "Здорово. Так себе.\\n\\n\n",
    "А что случилось?\\n\\n \n",
    "\"\"\"), ignore_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "class MLChitChat:\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(\"cointegrated/rut5-small-chitchat\")\n",
    "        self.generator = T5ForConditionalGeneration.from_pretrained(\"cointegrated/rut5-small-chitchat\")\n",
    "        self.ret_seq = 5\n",
    "\n",
    "    def __call__(self, texts) -> None:\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True)\n",
    "        with torch.no_grad():\n",
    "            hypotheses = self.generator.generate(\n",
    "                **inputs, \n",
    "                do_sample=True, num_beams=3, num_return_sequences=self.ret_seq,\n",
    "                repetition_penalty=2.5, top_k=500, temperature=1.13,\n",
    "                max_length=54, output_scores=True, return_dict_in_generate=True\n",
    "            )\n",
    "\n",
    "        ans_di = {}\n",
    "        tmp_li = []\n",
    "        hypotheses = zip(hypotheses.sequences, hypotheses.sequences_scores)\n",
    "        for i, (tens, score) in enumerate(hypotheses):\n",
    "            tmp_li.append((self.tokenizer.decode(tens, skip_special_tokens=True), score.item()))\n",
    "            if (i+1) % self.ret_seq == 0:\n",
    "                ans_di.update({(i+1)//self.ret_seq:tmp_li})\n",
    "                tmp_li = []\n",
    "        return ans_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLChitChat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [('Привет, спасибо.', -0.8036027550697327),\n",
       "  ('Привет, спасибо за помощь! Как у тебя все в порядке?',\n",
       "   -0.40717342495918274),\n",
       "  ('Привет, теперь все в порядке!', -0.6237233877182007),\n",
       "  ('хорошо, у тебя тоже все будет в порядке?', -0.5728853344917297)],\n",
       " 2: [('Мне так жаль это слышать.', -0.39720630645751953),\n",
       "  ('Что случилось?', -0.5411677360534668),\n",
       "  ('Ничего, ничего.', -0.4679516553878784),\n",
       "  ('О, ничего не случилось.', -0.4578409790992737)],\n",
       " 3: [('Что случилось?', -0.4788780212402344),\n",
       "  ('Здорово, я рад тебя видеть.', -0.5614637732505798),\n",
       "  ('Что случилось?', -0.4788780212402344),\n",
       "  ('Что случилось?', -0.4788780212402344)]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"\"\"Привет, как дела?\\n\\n\n",
    "Привет, всё хорошо)\\n\\n\n",
    "\"\"\",\n",
    "\"\"\"Привет, ты как?\\n\\n\n",
    "Здорово. Так себе.\\n\\n\n",
    "А что случилось?\\n\\n \n",
    "\"\"\",\n",
    "\"\"\"Привет, ты как? ))\\n\\n\n",
    "Здорово. Так себе (\\n\\n\n",
    "А что случилось :(\\n\\n \n",
    "\"\"\"]\n",
    "model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [('Пытаюсь кое-что попробовать.', -0.513427197933197),\n",
       "  ('Похоже, я занимаюсь спортом.', -0.4895842373371124),\n",
       "  ('Я ничем не занимаюсь.', -0.31862208247184753),\n",
       "  ('Работаю.', -0.4969412684440613),\n",
       "  ('Всего лишь на работе. Должно быть, тебе нужна помощь.',\n",
       "   -0.44850507378578186)]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Привет, как дела?\\n\\n\n",
    "Привет, всё хорошо.\\n\\n\n",
    "Чем занимаешься?\\n\\n \n",
    "\"\"\"\n",
    "model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persona_1_profile</th>\n",
       "      <th>persona_2_profile</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;span class=participant_1&gt;У меня любимая работ...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Ищу принца.&lt;br /&gt;Вед...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Пользователь 2: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;span class=participant_1&gt;Я работаю учителем&lt;b...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я бизнесмен&lt;br /&gt;У м...</td>\n",
       "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;span class=participant_1&gt;Я купила дом&lt;br /&gt;Я ...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я пою в караоке&lt;br /...</td>\n",
       "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;span class=participant_1&gt;я врач и женат&lt;br /&gt;...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я мальчик&lt;br /&gt;Я учу...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Пользователь 2: Здра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;span class=participant_1&gt;Я школьница.&lt;br /&gt;Я ...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я простоват.&lt;br /&gt;Лю...</td>\n",
       "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>&lt;span class=participant_1&gt;У меня 6 собак.&lt;br /...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я музыкант.&lt;br /&gt;Люб...</td>\n",
       "      <td>\"&lt;span class=participant_1&gt;Пользователь 1: При...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>&lt;span class=participant_1&gt;Я верная.&lt;br /&gt;Мне н...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я люблю рок&lt;br /&gt;Пиш...</td>\n",
       "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>&lt;span class=participant_1&gt;Я студент.&lt;br /&gt;Я уч...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Директор турфирмы.&lt;b...</td>\n",
       "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>&lt;span class=participant_1&gt;Моя мама живет со мн...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я воспитатель&lt;br /&gt;л...</td>\n",
       "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>&lt;span class=participant_1&gt;Я женат.&lt;br /&gt;Люблю ...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я люблю читать&lt;br /&gt;...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Пользователь 2: Прив...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10013 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       persona_1_profile  \\\n",
       "0      <span class=participant_1>У меня любимая работ...   \n",
       "1      <span class=participant_1>Я работаю учителем<b...   \n",
       "2      <span class=participant_1>Я купила дом<br />Я ...   \n",
       "3      <span class=participant_1>я врач и женат<br />...   \n",
       "4      <span class=participant_1>Я школьница.<br />Я ...   \n",
       "...                                                  ...   \n",
       "10008  <span class=participant_1>У меня 6 собак.<br /...   \n",
       "10009  <span class=participant_1>Я верная.<br />Мне н...   \n",
       "10010  <span class=participant_1>Я студент.<br />Я уч...   \n",
       "10011  <span class=participant_1>Моя мама живет со мн...   \n",
       "10012  <span class=participant_1>Я женат.<br />Люблю ...   \n",
       "\n",
       "                                       persona_2_profile  \\\n",
       "0      <span class=participant_2>Ищу принца.<br />Вед...   \n",
       "1      <span class=participant_2>Я бизнесмен<br />У м...   \n",
       "2      <span class=participant_2>Я пою в караоке<br /...   \n",
       "3      <span class=participant_2>Я мальчик<br />Я учу...   \n",
       "4      <span class=participant_2>Я простоват.<br />Лю...   \n",
       "...                                                  ...   \n",
       "10008  <span class=participant_2>Я музыкант.<br />Люб...   \n",
       "10009  <span class=participant_2>Я люблю рок<br />Пиш...   \n",
       "10010  <span class=participant_2>Директор турфирмы.<b...   \n",
       "10011  <span class=participant_2>Я воспитатель<br />л...   \n",
       "10012  <span class=participant_2>Я люблю читать<br />...   \n",
       "\n",
       "                                                dialogue  \n",
       "0      <span class=participant_2>Пользователь 2: Прив...  \n",
       "1      <span class=participant_1>Пользователь 1: Прив...  \n",
       "2      <span class=participant_1>Пользователь 1: Прив...  \n",
       "3      <span class=participant_2>Пользователь 2: Здра...  \n",
       "4      <span class=participant_1>Пользователь 1: Прив...  \n",
       "...                                                  ...  \n",
       "10008  \"<span class=participant_1>Пользователь 1: При...  \n",
       "10009  <span class=participant_1>Пользователь 1: Прив...  \n",
       "10010  <span class=participant_1>Пользователь 1: Прив...  \n",
       "10011  <span class=participant_1>Пользователь 1: Прив...  \n",
       "10012  <span class=participant_2>Пользователь 2: Прив...  \n",
       "\n",
       "[10013 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chats = pd.read_csv(\"TlkPersonaChatRus/dialogues.tsv\", sep='\\t')\n",
    "chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Пользователь 2: Привет) расскажи о себе\\n\\nПол...\n",
       "1        Пользователь 1: Привет!\\n\\nПользователь 2: При...\n",
       "2        Пользователь 1: Привет\\n\\nПользователь 1: Как ...\n",
       "3        Пользователь 2: Здравствуйте\\n\\nПользователь 2...\n",
       "4        Пользователь 1: Привет!\\n\\nПользователь 2: При...\n",
       "                               ...                        \n",
       "10008    \"Пользователь 1: Привет\\n\\nПользователь 2: При...\n",
       "10009    Пользователь 1: Привет)\\n\\nПользователь 1: Как...\n",
       "10010    Пользователь 1: Привет\\n\\nПользователь 2: прив...\n",
       "10011    Пользователь 1: Приветик)\\n\\nПользователь 2: П...\n",
       "10012    Пользователь 2: Привет! рада новому знакомству...\n",
       "Name: dialogue, Length: 10013, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chats['dialogue'] = chats['dialogue'].str.replace(\"</span><br />\", \"\\n\\n\")\n",
    "chats['dialogue'] = chats['dialogue'].str.replace(\"<span class=participant_2>\", \"\")\n",
    "chats['dialogue'] = chats['dialogue'].str.replace(\"<span class=participant_1>\", \"\")\n",
    "chats['dialogue'] = chats['dialogue'].str.replace(\"<br />\", \"\")\n",
    "chats['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "chats['dialogue'].str.split('\\n\\n').apply(lambda x: len(x)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirily/guildenstern/.env/lib/python3.8/site-packages/transformers/generation_utils.py:2343: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 2s ± 33.5 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model(chats['dialogue'].sample(10).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit model(chats['dialogue'].sample(50).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit model(chats['dialogue'].sample(25).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats['length'] = chats['dialogue'].str.split('\\n\\n').apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirily/guildenstern/.env/lib/python3.8/site-packages/transformers/generation_utils.py:2343: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 18s ± 19.2 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model(chats.loc[chats['length'].between(25, 50), 'dialogue'].sample(15).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guildenstern.src.libs.models.simple_bot import MLChitChat\n",
    "\n",
    "model = MLChitChat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'do_sample': True,\n",
       " 'num_beams': 3,\n",
       " 'num_return_sequences': 3,\n",
       " 'repetition_penalty': 2.5,\n",
       " 'top_k': 500,\n",
       " 'temperature': 1.11,\n",
       " 'max_length': 54,\n",
       " 'output_scores': True,\n",
       " 'return_dict_in_generate': True}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пользователь 2: Привет\\n\\nПользователь 1: О,приветики!Я Наталия,а как вас зовут?\\n\\nПользователь 2: Меня зовут Андрей, давайте ознакомиться\\n\\nПользователь 2: Знакомиться\\n\\nПользователь 1: С удовольствием! чем занимаетесь? может тоже любитеспорт? я без него ни дня не могу! ть\\n\\nПользователь 2: Я директор магазина, но знаете спорт я тоже обожаю, особенно водный\\n\\nПользователь 2: Кстати через месяц собираюсь на море поехать\\n\\nПользователь 2: Вы любите путешествовать?\\n\\nПользователь 1: Замечатьно как! Путешествовать просто обожаю! Познаватьновое- очень увлекательно. Можно сравнить разве чтос чтением книг! Любите читать?\\n\\nПользователь 2: Да знаете не особо, если читать то только учебники, по работе, и ещё по испанскому, вот изучаю его, думаюпригодиться\\n\\nПользователь 2: А у вас есть любимая книга?\\n\\nПользователь 1: Учебники тоже читаю,я учииельницей работаю.\\n\\nПользователь 2: Ого, круто, а какой предмет ведёте?\\n\\nПользователь 1: Есть, Ш. Бронте , Джейн Ейр . Вся про меня, молодуюучительницу в поисках своего принца 😊\\n\\nПользователь 2: В поисках? Значит вы не замужем?\\n\\nПользователь 1: Алгебру и геометрию.Неожидано наверное😊\\n\\nПользователь 1: Пока нет,принц еще не нашелся)а у вас есть семья?\\n\\nПользователь 2: Нет, я не женат\\n\\nПользователь 2: Вы любите музыку?\\n\\nПользователь 1: Да, конечно. Предпочитаю слушать рок. А вам какоенаправление близко по духу?\\n\\nПользователь 2: Я меломан) даже сам играю, на гитаре\\n\\nПользователь 2: А вы играете или играли на чем нибудь?\\n\\nПользователь 1: Вау! ну очень я вам скажу, очень! думаю нам пора прощаться, ато я еще придумаю себе, что нашла своего принца😊\\n\\nПользователь 2: Ахах хорошего вам дня!\\n\\nПользователь 1: Спасибо,и вам!\\n\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = chats['dialogue'].sample(1).to_list()[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message='__floordiv__ is deprecated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Мне нравится, что вы напоминаете об этом. Я надеюсь, что мне удалось это понять!',\n",
       "  -0.5270249247550964),\n",
       " ('Привет! Надеюсь, вы сможете найти себя в своем возрасте. Удачи!',\n",
       "  -0.5724456310272217),\n",
       " ('Я никогда не был на работе, и так все будет увлекательно! Пожалуйста!',\n",
       "  -0.7270546555519104)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"cointegrated/rut5-small-chitchat\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"cointegrated/rut5-small-chitchat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Хорошо. Какие у тебя дела?', 'Хорошо.', 'Великолепно. Хорошо.']\n",
      "tensor([-0.4027, -0.4113, -0.5968])\n"
     ]
    }
   ],
   "source": [
    "text = 'Привет! Расскажи, как твои дела?'\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    hypotheses = model.generate(\n",
    "        **inputs, \n",
    "        do_sample=True, top_k=500, num_return_sequences=3, num_beams=5,\n",
    "        top_p=0.95, repetition_penalty=2.5, temperature=1.15,\n",
    "        max_length=32, return_dict_in_generate=True, output_scores=True\n",
    "    )\n",
    "\n",
    "print(tokenizer.batch_decode(hypotheses.sequences, skip_special_tokens=True))\n",
    "print(hypotheses.sequences_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Да, работаю.\n",
      "-0.9424518942832947\n",
      "У меня все хорошо.\n",
      "-0.7928904294967651\n",
      "Работаю.\n",
      "-0.5319557785987854\n",
      "Что случилось?\n",
      "-0.5411679744720459\n",
      "Мне нужно поговорить с тобой.\n",
      "-0.4335683286190033\n",
      "Не знаю.\n",
      "-0.5221830606460571\n"
     ]
    }
   ],
   "source": [
    "text = [\"\"\"Привет, как дела?\\n\\n\n",
    "Привет, всё хорошо.\\n\\n\n",
    "Чем занимаешься?\\n\\n \n",
    "\"\"\",\n",
    "\"\"\"Привет, ты как?\\n\\n\n",
    "Здорово. Так себе.\\n\\n\n",
    "А что случилось?\\n\\n \n",
    "\"\"\"]\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True)\n",
    "with torch.no_grad():\n",
    "    hypotheses = model.generate(\n",
    "        **inputs, \n",
    "        do_sample=True, num_beams=3, num_return_sequences=3,\n",
    "        repetition_penalty=2.5, top_k=500, temperature=1.11,\n",
    "        max_length=54, output_scores=True, return_dict_in_generate=True\n",
    "    )\n",
    "hypotheses = zip(hypotheses.sequences, hypotheses.sequences_scores)\n",
    "for h, score in hypotheses:\n",
    "    print(tokenizer.decode(h, skip_special_tokens=True))\n",
    "    print(score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет, как дела? Привет, всё хорошо. Чем занимаешься?</s>',\n",
       " 'Привет, ты как? Здорово. Так себе. А что случилось?</s> <pad>']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = pd.read_csv('adjectives.csv', sep='\\t')['bare'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11941"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12450"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = []\n",
    "with open('nouns_anim_m.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        nouns.append(line.strip().replace('\\ufeff', ''))\n",
    "\n",
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29 ms ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open('guildenstern/.resource/nouns.txt') as f:\n",
    "    nouns = f.readlines()\n",
    "\n",
    "random.choice(nouns).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'стенной'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Коленька'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicks = []\n",
    "nicks.append(random.choice(adjectives).strip() + ' ' + random.choice(nouns).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guildenstern.src.libs.models.nickname_generator import generate_nickname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпохальный Вастьянов\n",
      "Эпохальный Вастьянов\n"
     ]
    }
   ],
   "source": [
    "nick = generate_nickname()\n",
    "print(nick)\n",
    "while nick in []:\n",
    "    nick = generate_nickname()\n",
    "print(nick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('guildenstern/.resource/nouns.txt', 'w') as f:\n",
    "    f.write('\\n'.join([a for a in nouns]))\n",
    "\n",
    "with open('guildenstern/.resource/adjectives.txt', 'w') as f:\n",
    "    f.write('\\n'.join([a for a in adjectives]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd20dea9e92d4121aaaf6d00bc1dc060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/377 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d928003a2ed447bb8d46706686ad5458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/235k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79c51e3d3dc4d4390245b017e3ae357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/457k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7dd86346ad34e5f929f1c232aef910f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89585bb7f65a43d287b49cb3f6b428c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9cf3dd3af64b8f8a81f158ad13ac77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/45.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_checkpoint = 'cointegrated/rubert-tiny-toxicity'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2toxicity(text, aggregate=True):\n",
    "    \"\"\" Calculate toxicity of a text (if aggregate=True) or a vector of toxicity aspects (if aggregate=False)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(model.device)\n",
    "        proba = torch.sigmoid(model(**inputs).logits).cpu().numpy()\n",
    "    if isinstance(text, str):\n",
    "        proba = proba[0]\n",
    "    if aggregate:\n",
    "        return 1 - proba.T[0] * (1 - proba.T[-1])\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93501186 0.04156357]\n"
     ]
    }
   ],
   "source": [
    "print(text2toxicity(['я люблю нигеров', 'я люблю африканцев'], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99914867 0.06166977 0.99970096]\n"
     ]
    }
   ],
   "source": [
    "print(text2toxicity(['соси хуй', 'от улыбки станет всем светлей', 'пидоры идут'], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9912947  0.08314562 0.99971104]\n"
     ]
    }
   ],
   "source": [
    "print(text2toxicity(['ля ты крыса', 'заткнись', 'придурок'], True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46cba23e8ff08ff3c4f743476507c4c8a48b1f37385563c99a3e275ccfef8284"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
